# -*- coding: utf-8 -*-
"""PytorchGradientes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hH1yfqMq46bhHRXrXSpc-j30x6LBAn-j
"""

import torch

# ============================================
# Autograd: diferenciación automática en Tensores
# ============================================
x = torch.randn(3, requires_grad=True)
y = x + 2
print(x)
print(y)
print(y.grad_fn)

z = y * y * 3
print(z)
z = z.mean()
print(z)

# Calculo del gradiente con retropropagación
z.backward()
print(x.grad)  # dz/dx

# Torch.autograd se basa en regla de la cadena
x = torch.randn(3, requires_grad=True)
y = x * 2
for _ in range(10):
    y = y * 2
print(y)
print(y.shape)

# Evaluar "gradiente" dy/dx en v
v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)
y.backward(v)
print(x.grad)

# Decirle a un tensor que deje generar gradientes
a = torch.randn(2, 2)
print(a.requires_grad)
b = (a * 3) / (a - 1)
print(b.grad_fn)

a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)

a = torch.randn(2, 2, requires_grad=True)
print(a.requires_grad)
b = a.detach()
print(b.requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)

# backward() acumula el gradiente en .grad
# .zero_() limpia el gradiente antes de comenzar
weights = torch.ones(4, requires_grad=True)

for epoch in range(3):
    model_output = (weights * 3).sum()
    model_output.backward()
    print(weights.grad)

    with torch.no_grad():
        weights -= 0.1 * weights.grad

    weights.grad.zero_()

print(weights)
print(model_output)